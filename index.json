[{"categories":["Hugo"],"content":"为什么要用Action来实现自动化部署hugo项目 不采用自动化部署，每次更新项目时，都需要在本地编译，然后上传整个public目录，比较麻烦 不采用自动化部署，没办法管理博客内容以及维护 ","date":"2023-11-12","objectID":"/workflow/:0:1","tags":["github","自动化部署"],"title":"如何利用GitHub的Action实现hugo项目自动化部署","uri":"/workflow/"},{"categories":["Hugo"],"content":"怎样实现 在github上创建 {{ username }}.github.io仓库，在main分支的基础上，新建source分支用于存放hugo的markdown文件，而main分支用于存放编译好后的文件 前期准备： 在本地，使用ssh工具创建公私钥 ssh-keygen -t rsa -N '' -f ./deploy-key -q 在本地我们会得到deploy-key和deploy-key.pub两个文件 现在需要将ssh密钥信息设置到仓库中： 添加deploy-key信息 deploy key的title随便填写即可，我们将生成的deploy-key.pub（即公钥）的内容填到key中 设置action的密钥： 新建一个actions secret 其中Name的内容填为ACTIONS_DEPLOY_KEY，Secret的内容填为我们生成的deploy-key（即私钥）的内容。 在source分支中添加.github/workflows/main.yml文件，用于定义Github Actions，其中main.yaml文件内容为 name: auto-update-page # 定义action名称 on: push: branches: - source # 当source分支出现push行为时，执行这个action jobs: deploy: runs-on: ubuntu-22.04 # 定义环境以及之后的行为 steps: - uses: actions/checkout@v4 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \"0.120.3\" extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir: ./public external_repository: plutolove233/plutolove233.github.io publish_branch: main # 将编译好的public里面的内容部署到对应仓库的分支上 设置完之后，我们查看仓库的Actions，我们能够看到： 看到如上内容，就意味着我们自动化部署的工作流完成了，我们可以尝试修改部分markdown文件，上传之后我们可以看到我们的博客内容也会发生改变 ","date":"2023-11-12","objectID":"/workflow/:0:2","tags":["github","自动化部署"],"title":"如何利用GitHub的Action实现hugo项目自动化部署","uri":"/workflow/"},{"categories":["机器学习之路"],"content":"基本概念： 对于强化学习，我们一般会分成智能体（agent），环境（通过智能体的状态和动作反馈信息）两大部分，我们现在介绍一些名词，从而有利于之后学习的理解。在这一部分，我们会结合一个3×3的网格寻路来形象化介绍下述概念。 我们最初位置是在（1，1），我们希望能够在最短的时间内到达蓝色网格位置。 State：agent相对于environment的状态。例如以s1表示网格（1，1），s9表示网格（3，3）。所有的状态会构成S（State Set）。 Action：每个状态可能采取的行动。例如向上（a1）、下（a3）、左（a4）、右（a2）走，这所有的动作会构成A（Action Set）。 state transition：agent从s1→s4的过程。 state transition probability：用条件概率的形式，表示转移的情况。例如p(s4|s1, a3)=0.5表示从状态s1执行动作a3（向下）到达状态s4的可能性是0.5（因为还可以向右到达s2的可能）。 Reward：一个数字，用于表示一个state transition行为所产生的激励。 forbidden area：进入但是会产生损失的区域（Reward\u003c0）。例如图示中的黄色区域。 Policy：让agent知道在某个状态该如何行动，也是用条件概率的形式来表示，例如π(a1|s1)=0，π(a2|s1)=0.5。同时我们应该还满足下述条件： $$ \\sum_{a\\in A}\\pi(a|s_j) = 1 $$ Trajectory：是整个状态行为链，例如s1(a2)→s2(a3)→s5(a3)→s8(a2)→s9(a4)→s8… Return：表示整个Trajectory上Reward之和 Discounted Rate：由上可知，一个Trajectory可能是无穷无尽的，所以我们需要定义γ来种折扣Reward，从而使整个过程的Return不会太大。 Discounted Return：表示整个Trajectory的折扣回报总和，具体表示为： $$ Discounted\\ Return=\\sum_{i=1}^\\infin \\gamma^{i-1}r_i $$ 我们可以知道，这个结果最终会收敛到一个值。并且γ越小，那么Discounted Return会与最早的reward相关（更近视），γ越大也就越远视 Episode：一个有限步的Trajectory，并且最终状态时terminal state（目标状态，即图中蓝色方块）。 ","date":"2023-11-10","objectID":"/concept/:0:1","tags":["RL","math"],"title":"强化学习数学知识总结","uri":"/concept/"},{"categories":["机器学习之路"],"content":"马尔可夫过程 随机过程 随机过程研究的对象是，随时间改变的随机现象。例如本例子中，时刻1在状态s1，时刻2可能就会在s2获s4。这一过程是随机的，我们通常用 $$ P(s_{t+1}|a_{t+1},s_{t}, a_{t}, s_{t-1}…a_1, s_0) $$ 来表示下一个时刻，状态的可能性 马尔可夫性质 当前状态只与上一个状态有关时，我们称这个过程具有马尔可夫性质，即 $$ P(s_{t+1}|a_{t+1},s_{t}, a_{t}, s_{t-1}…a_1, s_0)=P(s_{t+1}|a_{t+1}, s_t) $$ 在本例子中，每个时刻到达某个网格的状态可能完全取决于上一时刻所在状态和动作，所以我们这个网格寻路的例子是满足马尔可夫性质的。 几个新的概率 $$ P(s’|s, a)表示s\\stackrel{a}{\\longrightarrow}s’即从s执行动作a到达s’的概率\\\\ P(r |s, a)表示s\\stackrel{a}{\\longrightarrow}s’这一过程获得reward=r的概率\\\\ $$ 新的概念 State Value $$ v_{\\pi}(s)=\\mathbb{E}(G_t|S=s)\\\\ $$ 回报矩阵G：表示当前时刻所有状态所获得的discounted return $$ G_t=R_{t+1}+\\gamma G_{t+1} $$ 贝尔曼公式 我们将上述式子进行展开以及归纳，可得贝尔曼公式： $$ \\forall s\\in S，v_{\\pi}(s)=\\mathbb{E}(G_t|S_t=s)=\\mathbb{E}(R_{t+1}|S_t=s)+\\gamma\\mathbb{E}(G_{t+1}|S_t=s)\\\\ =\\sum_{a\\in A}\\pi(a|s)\\sum_rp(r|s,a)r + \\gamma\\sum_{s’}\\mathbb{E}(G_{t+1}|S_{t+1}=s’,S_t=s)P(s’|s)\\\\ =\\sum_{a\\in A}\\pi(a|s)\\sum_rp(r|s,a)r +\\gamma\\sum_{s’}v_{\\pi}(s’)\\sum_{a\\in A}P(s’|s, a)\\pi(a|s) $$ 我们将这个公式化成矩阵的形式 $$ \\mathbf{v_{\\pi}}=\\mathbf{r_{\\pi}}+\\gamma\\mathbf{P_{\\pi}}\\mathbf{v_{\\pi}} $$ 其中： $$ r_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)\\sum_rp(r|s,a)r\\\\ P_{\\pi}(s’|s)=\\sum_{a \\in A}p(s’|s,a)\\pi(a|s)，即s\\rarr s’的可能性\\\\ \\mathbf{P_{\\pi(i,j)}} = P_{\\pi}(s_j|s_i) $$ 我们该如何求解——求state value 求矩阵的逆 迭代的方式: 先随机设置v0初始值 利用 $$ \\mathbf{v_{k+1}}=\\mathbf{r_{\\pi}}+\\gamma\\mathbf{P_{\\pi}}\\mathbf{v_{k}} $$ 不断进行迭代，当k→∞时，vk→vπ ","date":"2023-11-10","objectID":"/concept/:0:2","tags":["RL","math"],"title":"强化学习数学知识总结","uri":"/concept/"},{"categories":["机器学习之路"],"content":"安装pytorch 我们以英伟达显卡为例，我们需要知道自己电脑对应cuda版本信息： 在控制台输入nvidia-smi我们可以看到对应cuda版本的信息内容： 从上图中，我们可知：当前我们CUDA的版本是11.2，之后我们去pytorch官方网页 查询对应pytorch的版本信息，按照给出的安装命令信息安装即可。 由于pytorch官网并没有cuda11.2的安装方法，所以我选择安装cuda11.1的环境 pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 我们可以通过一下代码来判断pytorch是否已经正确安装以及识别到显卡驱动 import torch print(torch.cuda.is_available()) print(torch.cuda.current_device()) print(torch.cuda.device_count()) print(torch.cuda.get_device_name(0)) 执行结果如下： ","date":"2023-11-09","objectID":"/linear/:0:1","tags":["pytorch","mechine learning"],"title":"pytorch实现线性拟合","uri":"/linear/"},{"categories":["机器学习之路"],"content":"编写代码 现在我们可以进行代码编写过程了 构造数据集，我们假设拟合的结果是 $$ y=w_1\\times x_1 + w_2\\times x_2 + b\\\\ 即y=\\begin{bmatrix} w_1 \u0026 w_2 \\\\ \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix}+b $$ 为了能够体现机器学习拟合的有效性，我们在生成数据点时会增加一些噪声，从而体现出数据的混乱 代码实现： def synthetic_data(w, b, num_examples): \"\"\"生成 y = Xw + b + 噪声。\"\"\" X = torch.normal(0, 1, (num_examples, len(w))) # X是均值为0，方差为1的随机数。有num_examples个样本，列就是len(w) y = torch.matmul(X, w) + b # Y是X与w的乘积（matmul == mm，矩阵相乘）加上偏差b y += torch.normal(0, 0.01, y.shape) # 加入一个噪音，均值为0，方差为0.01，形状与y相同 return X, y.reshape((-1, 1)) # 其中，y返回一个列向量。-1表示自动计算，1表示固定，即列向量为1 true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) 我们可以绘制图像，用于显示我们构造的数据： 然后我们将数据进行封装成Dataset class LinearDataSet(Dataset): def __init__(self, x, y): self.X = torch.FloatTensor(x) self.Y = torch.FloatTensor(y) def __getitem__(self, index): return self.X[index], self.Y[index] def __len__(self): return len(self.X) trainSet = LinearDataSet(features, labels) trainLoader = DataLoader(trainSet, batch_size=10, shuffle=True, pin_memory=True) 我们现在编写网络，以及编写一个训练过程： class LinearNetWork(nn.Module): def __init__(self, n_feature): super(LinearNetWork, self).__init__() self.layers = nn.Sequential( nn.Linear(n_feature, 1) # 定义一个最简单线性全连接层 ) def forward(self, x): y = self.layers(x) return y def Trainer(train_loader: DataLoader, model: LinearNetWork): criterion = nn.MSELoss() # 定义一个均方误差 optimizer = torch.optim.SGD(model.parameters(), lr=0.03) # 定义一个优化器 for i in range(epoch): model.train() for x, y in train_loader: optimizer.zero_grad() # 采用梯度下降，每次训练都需要将梯度信息清零 x, y = x.to(device), y.to(device) pred = model(x) loss = criterion(pred, y) print(\"loss=\", loss.detach().item()) loss.backward() # 梯度回退 optimizer.step() model = LinearNetWork(2).to(device) # 因为我们总共就两个变量，所以我们传入的特征信息为2 Trainer(trainLoader, model) print(model.state_dict()) 这样我们一个最简单的神经网络已经构成，我们执行上述代码查看网络每层的信息： ","date":"2023-11-09","objectID":"/linear/:0:2","tags":["pytorch","mechine learning"],"title":"pytorch实现线性拟合","uri":"/linear/"},{"categories":["机器学习之路"],"content":"总结 总的来说，这次这个任务还是比较简单的，我们构造一个简单的神经网络，完成了最简单的线性拟合的问题，我们可以在这里面一窥机器学习的基本过程。我们需要获取数据、数据处理、构造网络、进行训练、调整参数，然后不断循环往复，从而得到一个加好的结果。 ","date":"2023-11-09","objectID":"/linear/:0:3","tags":["pytorch","mechine learning"],"title":"pytorch实现线性拟合","uri":"/linear/"},{"categories":["机器学习之路"],"content":"问题描述 给定一个4×12的网格环境，如下图所示，其中黄色区域表示悬崖，我们不能经过，蓝色是我们的目标区域，我们希望能求出每个状态如何利用最少的步骤到达目的点。 ","date":"2023-11-10","objectID":"/cliffwalking/:0:1","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["机器学习之路"],"content":"编写工具类代码 # rl_utils.py def one_hot(index, num_size=10): return [1 if i == index else 0 for i in range(num_size)] class ReplayBuffer: def __init__(self, capacity): self.buffer = collections.deque(maxlen=capacity) def add(self, state, action, reward, next_state, done): self.buffer.append((state, action, reward, next_state, done)) def sample(self, batch_size): transitions = random.sample(self.buffer, batch_size) state, action, reward, next_state, done = zip(*transitions) return np.array(state), action, reward, np.array(next_state), done def size(self): return len(self.buffer) def train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size): return_list = [] for i in range(10): with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar: for i_episode in range(int(num_episodes / 10)): episode_return = 0 state = env.reset()[0] done = False while not done: action = agent.take_action(state) next_state, reward, done, _, _ = env.step(action) replay_buffer.add(state, action, reward, next_state, done) state = next_state episode_return += reward if replay_buffer.size() \u003e minimal_size: b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size) transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d} agent.update(transition_dict) return_list.append(episode_return) if (i_episode + 1) % 10 == 0: pbar.set_postfix({'episode': '%d' % (num_episodes / 10 * i + i_episode + 1), 'return': '%.3f' % np.mean(return_list[-10:])}) pbar.update(1) return return_list ","date":"2023-11-10","objectID":"/cliffwalking/:0:2","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["机器学习之路"],"content":"制定规则 我们规定，每走一步所得的reward是-1，走进悬崖的reward是-100，到达目标地点的reward是100。 我们编写环境代码如下： # grid_world.py class GridWorld: \"\"\" 悬崖问题的环境建立 \"\"\" def __init__(self, row=4, col=12): self.action_num = 4 self.col = col self.row = row self.x = row - 1 self.y = 0 self.statue_dim = self.row*self.col self.action_dim = 4 def step(self, action) -\u003e (list, float, bool, bool, dict): action = int(action) # 假设一个3*3的情景，目的是中间方块，求最短距离 self.x = min(self.row - 1, max(0, self.x + change[action][0])) self.y = min(self.col - 1, max(0, self.y + change[action][1])) next_state = self.x * self.col + self.y reward = -1 done = False if self.x == self.row - 1 and self.y \u003e 0: done = True if self.y != self.col - 1: # enter into hole reward = -100 else: # reach the final reward = 100 nextState = np.array(rl.one_hot(next_state, num_size=self.row * self.col)) return nextState, reward, done, False, {} def reset(self) -\u003e (list, dict): self.x = self.row - 1 self.y = 0 label = self.x * self.col + self.y state = np.array(rl.one_hot(label, num_size=self.row * self.col)) return state, {} 在上述代码中，我们实现了环境的定义，同时，我们注意到，我们返回的状态采用了独热编码，这也就意味着，之后我们构建网络时，输入的维度应该是48。 ","date":"2023-11-10","objectID":"/cliffwalking/:0:3","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["机器学习之路"],"content":"编写神经网络用来拟合QSA target 我们需要知道，这个网络的输入是48，输出应该是action个数。 # grid_world.py class CliffWalkNet(nn.Module): \"\"\" 悬崖问题神经网络构造 \"\"\" def __init__(self, in_dim, hidden_dim, out_dim): super(CliffWalkNet, self).__init__() self.layer = nn.Sequential( nn.Linear(in_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim) ) def forward(self, x): return self.layer(x) 在里面使用三个全连接层，以及两个隐藏层。 ","date":"2023-11-10","objectID":"/cliffwalking/:0:4","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["机器学习之路"],"content":"编写DQN核心代码 我们每个动作采用ε贪婪策略，以及规定每次操作执行的更新方法 class CliffWalkDQLearning: def __init__(self, in_dim, hidden_dim, action_dim): self.ActionNumber = action_dim self.Q_net = CliffWalkNet(in_dim, hidden_dim, action_dim).to(device) self.Q_target = CliffWalkNet(in_dim, hidden_dim, action_dim).to(device) self.optimizer = torch.optim.Adam(self.Q_net.parameters(), lr=learning_rate) self.criterion = nn.MSELoss() self.count = 0 self.QSA = [[0, 0, 0, 0] for _ in range(in_dim)] def take_action(self, state): if np.random.random() \u003c epsilon: return np.random.randint(self.ActionNumber) state = torch.tensor(state, dtype=torch.float).to(device) return self.Q_net(state).argmax().item() def update(self, transition: dict): # load data from dictionary states_ = torch.tensor(transition.get(\"states\"), dtype=torch.float).to(device) actions_ = torch.tensor(transition.get(\"actions\")).view(-1, 1).to(device) rewards_ = torch.tensor(transition.get(\"rewards\"), dtype=torch.float).view(-1, 1).to(device) next_states_ = torch.tensor(transition.get(\"next_states\"), dtype=torch.float).to(device) dones_ = torch.tensor(transition.get(\"dones\"), dtype=torch.float).view(-1, 1).to(device) q_values = self.Q_net(states_).gather(1, actions_) # 每列收集对应actions_拟合的value值 max_qvalue = self.Q_target(next_states_).max(1)[0].view(-1, 1) q_target = rewards_ + gamma * max_qvalue * (1-dones_) loss = self.criterion(q_values, q_target) self.optimizer.zero_grad() loss.backward() self.optimizer.step() if self.count % update_times == 0: net_state_dict = self.Q_net.state_dict() self.Q_target.load_state_dict(net_state_dict) self.count += 1 ","date":"2023-11-10","objectID":"/cliffwalking/:0:5","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["机器学习之路"],"content":"执行代码，进行模型训练 我们可以直接利用rl_utils里面的（off_policy）工具，进行DQN的训练 env = GridWorld() dim = env.row*env.col agent = CliffWalkDQLearning(in_dim=dim, hidden_dim=int(dim/2), action_dim=4) buffer = rl.ReplayBuffer(1000) random.seed(0) np.random.seed(0) torch.manual_seed(0) return_list = rl.train_off_policy_agent(env, agent, num_episodes, buffer, minimal_size, batch_size) episodes_list = list(range(len(return_list))) plt.plot(episodes_list, return_list) plt.xlabel('Episodes') plt.ylabel('Returns') plt.title('DQN on {}'.format(\"Cliff Walking\")) plt.show() ","date":"2023-11-10","objectID":"/cliffwalking/:0:6","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["机器学习之路"],"content":"模型保存 torch.save({\"CliffWalking\": agent.Q_target.state_dict()}, \"./cliff_walking.pth\") # load model model = CliffWalkNet(in_dim=dim, hidden_dim=int(dim/2), out_dim=4).to(device) state_dict = torch.load(\"./cliff_walking.pth\")[\"CliffWalking\"] model.load_state_dict(state_dict) res = [[0, 0, 0, 0] for _ in range(dim)] for i in range(4): for j in range(12): state = rl.one_hot(i*12+j, dim) input_ = torch.tensor(state, dtype=torch.float).to(device) target = model(input_).cpu().detach().numpy().tolist() res[i*12+j] = target # print(\"state(%d,%d):\" % (i, j)) # print(target) # print() print_agent(res, env, action_graph, list(range(37, 47)), [47]) 我们将训练好的模型保存，之后导入model，这样我们就可以避免重复的模型训练，直接计算每个结点的QSA，寻找每个状态的action最大值，从而得出训练后的策略结果。 由上述结果，我们发现每个状态都已经找到了最优的策略，这个模型的结果还算可以。 ","date":"2023-11-10","objectID":"/cliffwalking/:0:7","tags":["pytorch","DQN","RL"],"title":"DQN算法示例：CliffWalking问题","uri":"/cliffwalking/"},{"categories":["Docker"],"content":"xxx ","date":"2023-11-14","objectID":"/docker-cmd/:0:0","tags":null,"title":"docker常用命令介绍","uri":"/docker-cmd/"},{"categories":["Docker"],"content":"什么是docker docker可以说是一门技术、一种软件，它能够帮助我们将程序进行打包，从而方便后续的项目部署以及维护。 ","date":"2023-11-14","objectID":"/introduce/:0:1","tags":["docker"],"title":"docker介绍以及安装步骤说明","uri":"/introduce/"},{"categories":["Docker"],"content":"为什么要用docker 场景：做项目开发的同学经常会遇到一个问题，就是这个项目能够在本地上运行，但是换一台电脑却会报各种各样的错误，这个时候不得不面对环境配置这一极其麻烦的问题 所以我们说，docker将过去的程序和系统环境的绑定关系给打破了，只要我们的电脑安装了docker，我就可以通过拉取云端的镜像，无需关注环境的搭建就可以实现跨平台的应用部署。我们不再需要面对繁杂的应用上云出现的环境搭建问题，就可以轻松地使用服务。 ","date":"2023-11-14","objectID":"/introduce/:0:2","tags":["docker"],"title":"docker介绍以及安装步骤说明","uri":"/introduce/"},{"categories":["Docker"],"content":"怎样安装docker 安装docker： Windows系统： 在Windows系统，我们需要确保自己的电脑安装了WSL 以管理员身份打开CMD，并敲入： wsl --install 官方文档：安装 WSL | Microsoft Learn ，可查阅官方文档获得更多的信息。 之后我们前往docker-desktop官方网站Docker Desktop: The #1 Containerization Tool for Developers | Docker 下载最新的docker-desktop即可。 需要注意的是，在使用安装好docker-desktop之后，我们需要修改几个配置 调整docker虚拟机的位置，不然根据初始设置，会把C盘占满 修改docker的镜像源信息，不然在docker pull拉取镜像时，会特别慢： 将其中的内容替换成下述信息： { \"builder\": { \"gc\": { \"defaultKeepStorage\": \"20GB\", \"enabled\": true } }, \"experimental\": false, \"features\": { \"buildkit\": true }, \"registry-mirrors\": [ \"http://hub-mirror.c.163.com\", \"https://mirror.ccs.tencentyun.com\" ] } 或者直接修改C:\\Users\\{{ USERNAME }}\\.docker\\daemon.json内容为上述信息，重启docker。 判断是否安装成功，在CMD中，敲入： docker version 出现如下信息即可： docker之所以需要在Linux环境下使用，是源于在使用go进行编写docker时，需要借助Linux系统的Namespace和Cgroup隔离机制，所以我们在Windows上运行docker实质上是在Windows上安装了一个虚拟机WSL，在虚拟机的内部执行docker Ubuntu系统： 更新索引：sudo apt-get update 安装：sudo apt install docker.io 修改镜像源：在/etc/docker目录下创建daemon.json，在里面添加： { \"registry-mirrors\": [ \"http://hub-mirror.c.163.com\", \"https://mirror.ccs.tencentyun.com\" ] } 即可 判断是否安装成功，在终端输入：docker --version，出现 即可 CentOS/aliyun服务器： 卸载旧版（如果你之前没有安装过，可跳过这一步） yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装依赖包：yum install -y yum-utils 为了防止是从国外的源下载，所以配置阿里云地址：yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新依赖：yum update 安装docker：yum install docker-ce docker-ce-cli containerd.io 配置docker镜像源：同样的在/etc/docker目录下创建daemon.json文件，填入镜像源地址信息即可。 启动docker服务：systemctl start docker 判断是否安装完成：控制台输入docker info，看到镜像源信息更改即为安装成功 当然你可以也可以在其他平台上使用docker info来判断镜像源的地址信息是否成功修改 ","date":"2023-11-14","objectID":"/introduce/:0:3","tags":["docker"],"title":"docker介绍以及安装步骤说明","uri":"/introduce/"},{"categories":["Docker"],"content":"一些最基本的概念 仓库Registry：你可以理解为类似代码仓库的东西，只是这里面存放的不是一段段代码，而是一堆镜像，我们可以通过一些指令来获得或上传镜像信息。 镜像Image：它是一种模板，具有创建docker容器的指令。并且一种镜像是得基于另一种镜像才能构建。我们可以把这个理解为虚拟机安装所需的iso文件，只要我有这个iso文件，我就可以打造多个一样的虚拟机环境。当然我们还可以在这个iso的基础上附带一些东西，构成一个新的iso。例如centos分为minimal（最基础的）以及desktop（带有桌面UI的），desktop就是基于minimal，同时又添加了新的东西形成的新的iso文件。 容器Container：它是一种运行实例，每个容器之间是相互隔离互不影响的。同样的，我们举个例子，当你拥有一个iso文件时，你不把它放在VMware中产生一个虚拟机实例，那么这个iso文件也就毫无用处。而你使用同一个iso文件创建的不同虚拟机之间也是不会有任何影响，你不会发现，在A虚拟机中创建了一个文件，B虚拟机中也可以找到。 如果上述的例子还是不能让你理解这三者之间的关系和实际的作用，我们可以拿日常开发做一个例子。仓库Registry就是github.com；镜像就是github.com中的pytorch项目代码，我们可以利用pytorch本身的东西创建一个新的例如predict的项目代码，当然我们也可以上传到github；容器就相当于我们执行这段代码的进程，每个进程之间不会相互影响（不考虑使用共享内存，管道等方式实现进程间的控制）。 ","date":"2023-11-14","objectID":"/introduce/:0:4","tags":["docker"],"title":"docker介绍以及安装步骤说明","uri":"/introduce/"},{"categories":["Docker"],"content":"总结 在这一章中，我们大概介绍了docker的一些基本信息和安装方式，在下一章我们会在ubuntu系统上介绍docker的基本操作。 ","date":"2023-11-14","objectID":"/introduce/:0:5","tags":["docker"],"title":"docker介绍以及安装步骤说明","uri":"/introduce/"},{"categories":null,"content":"个人介绍 出生于2002年6月，喜欢go语言，LOL的蒟蒻。 目前在华北电力大学就读软件工程本科学业。 目前正在学习强化学习，希望能够通过编写个人博客来增加我学习的动力，以及记录我学习的过程。 博客会更新什么 本科以及之后研究生学习的过程和结果 go语言相关的内容，会包括最新特性，以及我觉得有用的东西 docker应用示例，我希望通过几个简短的章节能够让你了解docker如何部署一个项目 或许还会有一些每周心得体会什么的？ 我也希望能够有人在阅读我的博客之后，发送email（yizhigopher@163.com）和我交流，希望能够在各位大佬变得更强，无限进步。 ","date":"2023-11-08","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"这是我的第一份博客，用于测试 package main import \"fmt\" func main(){ fmt.Println(\"Hello World\") } $$ \\sum_{i=0}^{100}i = 5050 $$ ","date":"2023-11-08","objectID":"/first_post/:0:0","tags":["hugo"],"title":"First_post","uri":"/first_post/"}]